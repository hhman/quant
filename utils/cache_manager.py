#!/usr/bin/env python3
"""
Cache ç®¡ç†å™¨ - Parquet ç‰ˆï¼ˆæ™ºèƒ½æ¨¡å¼ï¼‰
æ–‡ä»¶åå³å…ƒæ•°æ®ï¼Œæ— ç‹¬ç«‹ metadata æ–‡ä»¶
æ™ºèƒ½åˆ¤æ–­ï¼šè‡ªåŠ¨è¿½åŠ æ–°åˆ—ã€æ›¿æ¢å·²å­˜åœ¨åˆ—ã€åˆ›å»ºæ–°æ–‡ä»¶

ç‰¹æ®ŠåŠŸèƒ½ï¼š
- æ”¶ç›Šç‡æ•°æ®è‡ªåŠ¨ä¿å­˜ä¸ºallå¸‚åœºï¼Œæ”¯æŒè·¨å¸‚åœºå¤ç”¨
- è¯»å–æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶å¤ç”¨allå¸‚åœºæ”¶ç›Šç‡æ–‡ä»¶
"""

from pathlib import Path
from typing import Optional
import pandas as pd


class CacheManager:
    """
    Parquet Cache ç®¡ç†å™¨

    ç‰¹æ€§ï¼š
    - æ–‡ä»¶åç¼–ç  market, start_date, end_date, type
    - æ™ºèƒ½æ¨¡å¼ï¼šè‡ªåŠ¨åˆ¤æ–­è¿½åŠ /æ›¿æ¢/åˆ›å»º
    - æ”¯æŒé«˜æ•ˆçš„éƒ¨åˆ†åˆ—è¯»å–
    - æ— ç‹¬ç«‹ metadata æ–‡ä»¶
    - âœ¨ æ”¶ç›Šç‡æ•°æ®è·¨å¸‚åœºå¤ç”¨ï¼ˆè‡ªåŠ¨ä¿å­˜ä¸ºallå¸‚åœºï¼‰
    """

    CACHE_DIR = Path(".cache")

    def __init__(
        self,
        market: str,
        start_date: str,  # YYYY-MM-DD
        end_date: str,  # YYYY-MM-DD
    ):
        """
        åˆå§‹åŒ– Cache Manager

        Args:
            market: å¸‚åœºæ ‡è¯†
            start_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        self.market = market
        self.start_date = start_date
        self.end_date = end_date

        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ (YYYYMMDD)
        self.start_date_compact = start_date.replace("-", "")
        self.end_date_compact = end_date.replace("-", "")

        # åˆ›å»º cache ç›®å½•
        self.CACHE_DIR.mkdir(exist_ok=True)

    # ========================================================================
    # æ–‡ä»¶è·¯å¾„ç®¡ç†
    # ========================================================================

    def get_parquet_path(self, data_type: str) -> Path:
        """
        ç”Ÿæˆ Parquet æ–‡ä»¶è·¯å¾„

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†

        Returns:
            Parquet æ–‡ä»¶å®Œæ•´è·¯å¾„
        """
        filename = f"{self.market}_{self.start_date_compact}_{self.end_date_compact}__{data_type}.parquet"
        return self.CACHE_DIR / filename

    @staticmethod
    def parse_filename(filename: str) -> dict:
        """
        è§£ææ–‡ä»¶åï¼Œæå–å‚æ•°ä¿¡æ¯

        Args:
            filename: æ–‡ä»¶å

        Returns:
            å‚æ•°å­—å…¸ {'market': str, 'start_date': str, 'end_date': str, 'type': str}
        """
        name = filename.replace(".parquet", "")
        parts = name.split("__")

        if len(parts) != 2:
            raise ValueError(f"æ— æ•ˆçš„æ–‡ä»¶åæ ¼å¼: {filename}")

        info_part = parts[0].split("_")
        if len(info_part) < 3:
            raise ValueError(f"æ— æ•ˆçš„æ–‡ä»¶åæ ¼å¼: {filename}")

        market = info_part[0]
        start_compact = info_part[1]
        end_compact = info_part[2]

        start_date = f"{start_compact[:4]}-{start_compact[4:6]}-{start_compact[6:8]}"
        end_date = f"{end_compact[:4]}-{end_compact[4:6]}-{end_compact[6:8]}"

        return {
            "market": market,
            "start_date": start_date,
            "end_date": end_date,
            "type": parts[1],
        }

    # ========================================================================
    # æ•°æ®å†™å…¥ï¼ˆæ™ºèƒ½æ¨¡å¼ï¼‰
    # ========================================================================

    def write_dataframe(
        self,
        df: pd.DataFrame,
        data_type: str,
        compression: str = "snappy",
        verbose: bool = True,
    ) -> None:
        """
        å†™å…¥ DataFrame åˆ° Parquetï¼ˆæ™ºèƒ½æ¨¡å¼ï¼‰

        è‡ªåŠ¨è¡Œä¸ºï¼ˆåŸºäºä¸¥æ ¼çš„è¡¨è¾¾å¼å­—ç¬¦ä¸²åŒ¹é…ï¼‰ï¼š
        - æ”¶ç›Šç‡æ•°æ®ï¼ˆreturnsï¼‰ï¼š
          - è‡ªåŠ¨ä¿å­˜ä¸ºallå¸‚åœºï¼Œæ”¯æŒè·¨å¸‚åœºå¤ç”¨
          - æ£€æŸ¥æ•°æ®æ˜¯å¦ç›¸åŒï¼Œç›¸åŒåˆ™è·³è¿‡å†™å…¥
          - åŸå› ï¼šæ”¶ç›Šç‡æ˜¯ç¡®å®šæ€§è®¡ç®—ï¼Œç›¸åŒè¾“å…¥å¿…å®šäº§ç”Ÿç›¸åŒè¾“å‡º
        - é£æ ¼æ•°æ®ï¼ˆstylesï¼‰ï¼š
          - è‡ªåŠ¨ä¿å­˜ä¸ºallå¸‚åœºï¼Œæ”¯æŒè·¨å¸‚åœºå¤ç”¨
          - æ£€æŸ¥æ•°æ®æ˜¯å¦ç›¸åŒï¼Œç›¸åŒåˆ™è·³è¿‡å†™å…¥
          - åŸå› ï¼šé£æ ¼æ•°æ®å¯¹äºæ‰€æœ‰å¸‚åœºç›¸åŒï¼Œé¿å…é‡å¤è®¡ç®—
        - å…¶ä»–æ•°æ®ï¼ˆfactor_raw, factor_std, neutralizedç­‰ï¼‰ï¼š
          - æ–‡ä»¶ä¸å­˜åœ¨ -> åˆ›å»ºæ–°æ–‡ä»¶
          - æ–‡ä»¶å­˜åœ¨ -> æ™ºèƒ½åˆå¹¶ï¼š
            * ç›¸åŒè¡¨è¾¾å¼ â†’ æ›¿æ¢ï¼ˆé‡æ–°è®¡ç®—ï¼‰
            * ä¸åŒè¡¨è¾¾å¼ â†’ è¿½åŠ ï¼ˆæ–°å› å­ï¼‰
            * æœªè¯·æ±‚çš„å·²å­˜åœ¨å› å­ â†’ ä¿ç•™ï¼ˆä¸åˆ é™¤ï¼‰

        Args:
            df: è¦å†™å…¥çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            compression: å‹ç¼©æ–¹å¼ ('snappy', 'gzip', 'brotli', 'lz4')
            verbose: æ˜¯å¦æ‰“å°æ“ä½œä¿¡æ¯
        """
        # ç‰¹æ®Šæ•°æ®å¤„ç†ï¼šreturns å’Œ styles ç»Ÿä¸€ä¿å­˜ä¸ºallå¸‚åœº
        if data_type in ["returns", "styles"]:
            all_cache_mgr = CacheManager("all", self.start_date, self.end_date)
            path = all_cache_mgr.get_parquet_path(data_type)
            if verbose:
                data_name = "æ”¶ç›Šç‡" if data_type == "returns" else "é£æ ¼æ•°æ®"
                print(f"    ğŸ’¾ {data_name}ä¿å­˜ä¸ºallå¸‚åœº: {path.name}")
                print("    âš¡ å…¶ä»–å¸‚åœºå¯å¤ç”¨æ­¤æ–‡ä»¶")
        else:
            path = self.get_parquet_path(data_type)

        if path.exists():
            # returns å’Œ stylesï¼šæ–‡ä»¶å­˜åœ¨å³å¤ç”¨ï¼ˆè·¨å¸‚åœºå…±äº«ï¼Œç¡®å®šæ€§è®¡ç®—ï¼‰
            if data_type in ["returns", "styles"]:
                if verbose:
                    data_name = "æ”¶ç›Šç‡" if data_type == "returns" else "é£æ ¼æ•°æ®"
                    print(f"    âœ… {data_name}æ–‡ä»¶å·²å­˜åœ¨ï¼Œå¤ç”¨å·²æœ‰æ–‡ä»¶")
                return  # ç›´æ¥è·³è¿‡å†™å…¥

            # å…¶ä»–æ•°æ®ï¼šæ™ºèƒ½åˆå¹¶
            if verbose:
                print("    ğŸ“„ æ£€æµ‹åˆ°å·²æœ‰æ–‡ä»¶ï¼Œæ‰§è¡Œæ™ºèƒ½åˆå¹¶...")

            # è¯»å–ç°æœ‰æ•°æ®
            existing_df = pd.read_parquet(path)

            # æ™ºèƒ½åˆå¹¶
            result_df, merge_info = self._smart_merge(existing_df, df, verbose)

            # å†™å…¥åˆå¹¶åçš„æ•°æ®
            result_df.to_parquet(path, compression=compression, index=True)

            if verbose:
                self._log_merge_result(merge_info)
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼šç›´æ¥åˆ›å»º
            if verbose:
                print("    ğŸ’¾ åˆ›å»ºæ–°æ–‡ä»¶")

            # å†™å…¥
            df.to_parquet(path, compression=compression, index=True)

    # ========================================================================
    # æ™ºèƒ½åˆå¹¶è¾…åŠ©æ–¹æ³•
    # ========================================================================

    def _smart_merge(
        self,
        existing_df: pd.DataFrame,
        new_df: pd.DataFrame,
        verbose: bool = True,
    ) -> tuple[pd.DataFrame, dict]:
        """
        æ™ºèƒ½åˆå¹¶ä¸¤ä¸ª DataFrameï¼ˆåŸºäºä¸¥æ ¼çš„è¡¨è¾¾å¼å­—ç¬¦ä¸²åŒ¹é…ï¼‰

        æ ¸å¿ƒåŸåˆ™ï¼š
        - å› å­èº«ä»½ = å®Œæ•´çš„è¡¨è¾¾å¼å­—ç¬¦ä¸²
        - ç›¸åŒè¡¨è¾¾å¼ â†’ æ›¿æ¢ï¼ˆé‡æ–°è®¡ç®—ï¼‰
        - ä¸åŒè¡¨è¾¾å¼ â†’ è¿½åŠ ï¼ˆæ–°å› å­ï¼‰
        - æœªè¯·æ±‚çš„å·²å­˜åœ¨å› å­ â†’ ä¿ç•™ï¼ˆä¸åˆ é™¤ï¼‰

        Args:
            existing_df: ç°æœ‰çš„æ•°æ®
            new_df: æ–°çš„æ•°æ®
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            (åˆå¹¶åçš„DataFrame, åˆå¹¶ä¿¡æ¯å­—å…¸)
        """
        existing_cols = set(existing_df.columns)
        new_cols = set(new_df.columns)

        # åˆ†ç±»ï¼ˆåŸºäºå­—ç¬¦ä¸²ç²¾ç¡®åŒ¹é…ï¼‰
        to_replace = existing_cols & new_cols  # äº¤é›†ï¼šç›¸åŒè¡¨è¾¾å¼ â†’ æ›¿æ¢
        to_append = new_cols - existing_cols  # å·®é›†ï¼šä¸åŒè¡¨è¾¾å¼ â†’ è¿½åŠ 
        to_keep = existing_cols - new_cols  # å·®é›†ï¼šæœªè¯·æ±‚ â†’ ä¿ç•™

        # åˆå¹¶
        result_df = existing_df.drop(columns=list(to_replace))
        result_df = pd.concat([result_df, new_df], axis=1)

        return result_df, {
            "replaced": list(to_replace),
            "appended": list(to_append),
            "kept": list(to_keep),
        }

    def _log_merge_result(self, merge_info: dict) -> None:
        """
        æ‰“å°åˆå¹¶ç»“æœçš„å‹å¥½æ—¥å¿—

        Args:
            merge_info: åˆå¹¶ä¿¡æ¯å­—å…¸
        """
        replaced = merge_info["replaced"]
        appended = merge_info["appended"]
        kept = merge_info["kept"]

        # æ‰“å°æ›¿æ¢ä¿¡æ¯
        if replaced:
            print(f"    ğŸ”„ æ›´æ–°å·²æœ‰å› å­ ({len(replaced)}ä¸ª):")
            for col in replaced:
                print(f"       - {col}")

        # æ‰“å°è¿½åŠ ä¿¡æ¯
        if appended:
            print(f"    â• è¿½åŠ æ–°å› å­ ({len(appended)}ä¸ª):")
            for col in appended:
                print(f"       - {col}")

        # æ‰“å°ä¿ç•™ä¿¡æ¯
        if kept:
            print(f"    âœ… ä¿ç•™å·²æœ‰å› å­ ({len(kept)}ä¸ª):")
            for col in kept:
                print(f"       - {col}")

        # å¦‚æœæ²¡æœ‰ä»»ä½•å˜åŒ–
        if not replaced and not appended:
            print("    âœ… å› å­æ— å˜åŒ–ï¼Œè·³è¿‡å†™å…¥")

    # ========================================================================
    # æ•°æ®è¯»å–
    # ========================================================================

    def read_dataframe(
        self,
        data_type: str,
        columns: Optional[list[str]] = None,
    ) -> pd.DataFrame:
        """
        ä» Parquet è¯»å– DataFrame

        ç‰¹æ®Šå¤„ç†ï¼š
        - æ”¶ç›Šç‡æ•°æ®ä¼˜å…ˆå¤ç”¨allå¸‚åœºæ–‡ä»¶
        - é£æ ¼æ•°æ®ä¼˜å…ˆå¤ç”¨allå¸‚åœºæ–‡ä»¶
        - æ”¯æŒéƒ¨åˆ†åˆ—è¯»å–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        - å‹å¥½çš„é”™è¯¯æç¤º

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            columns: è¦è¯»å–çš„åˆ—ï¼ˆNone=å…¨éƒ¨ï¼‰

        Returns:
            DataFrame

        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: è¯·æ±‚çš„åˆ—ä¸å­˜åœ¨
        """
        # returns å’Œ styles æ•°æ®ä¼˜å…ˆæŸ¥æ‰¾allå¸‚åœºæ–‡ä»¶ï¼Œæ”¯æŒè·¨å¸‚åœºå¤ç”¨
        if data_type in ["returns", "styles"]:
            all_path = (
                self.CACHE_DIR
                / f"all_{self.start_date_compact}_{self.end_date_compact}__{data_type}.parquet"
            )
            if all_path.exists():
                if self.market != "all":
                    data_name = "æ”¶ç›Šç‡" if data_type == "returns" else "é£æ ¼æ•°æ®"
                    print(f"    âš¡ å¤ç”¨allå¸‚åœº{data_name}æ•°æ® (è·¨å¸‚åœºå¤ç”¨)")
                path = all_path
            else:
                path = self.get_parquet_path(data_type)
        else:
            path = self.get_parquet_path(data_type)

        if not path.exists():
            raise FileNotFoundError(
                f"âŒ Cache æ–‡ä»¶ä¸å­˜åœ¨: {path}\n   è¯·å…ˆè¿è¡Œ step1 ç”Ÿæˆ cache"
            )

        # æ£€æŸ¥è¯·æ±‚çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœæŒ‡å®šäº†åˆ—ï¼‰
        if columns is not None:
            try:
                import pyarrow.parquet as pq

                schema = pq.read_schema(path)
                existing_cols = set(schema.names)
                missing_cols = set(columns) - existing_cols

                if missing_cols:
                    available_cols = sorted(existing_cols)
                    raise ValueError(
                        f"âŒ è¯·æ±‚çš„å› å­ä¸å­˜åœ¨:\n"
                        f"   ç¼ºå¤±: {sorted(missing_cols)}\n"
                        f"   å¯ç”¨: {available_cols}\n"
                        f"   æ–‡ä»¶: {path.name}\n"
                        f"\n"
                        f"ğŸ’¡ å»ºè®®:\n"
                        f"   - å…ˆè¿è¡Œ step1 ç”Ÿæˆç¼ºå¤±çš„å› å­:\n"
                        f'     python step1/cli.py --factor-formulas "{" ".join(missing_cols)}" ...'
                    )
            except Exception as e:
                # å¦‚æœè¯»å–schemaå¤±è´¥ï¼Œç›´æ¥å°è¯•è¯»å–æ•°æ®
                if "requested columns not present" in str(e):
                    raise e

        if columns is None:
            return pd.read_parquet(path)
        else:
            # éƒ¨åˆ†åˆ—è¯»å–ï¼ˆé«˜æ•ˆï¼‰
            return pd.read_parquet(path, columns=columns)

    def read_columns(
        self,
        columns: list[str],
        data_type: str,
    ) -> pd.DataFrame:
        """
        åªè¯»å–æŒ‡å®šçš„åˆ—ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰

        Args:
            columns: åˆ—ååˆ—è¡¨
            data_type: æ•°æ®ç±»å‹æ ‡è¯†

        Returns:
            DataFrameï¼ˆåªåŒ…å«æŒ‡å®šçš„åˆ—ï¼‰
        """
        return self.read_dataframe(data_type, columns=columns)

    # ========================================================================
    # å·¥å…·æ–¹æ³•
    # ========================================================================

    def check_columns(
        self,
        data_type: str,
        required_columns: list[str],
        verbose: bool = False,
    ) -> dict:
        """
        æ£€æŸ¥è¯·æ±‚çš„åˆ—æ˜¯å¦éƒ½å­˜åœ¨ï¼ˆä¼˜åŒ–ï¼šåªè¯»å…ƒæ•°æ®ï¼Œä¸è¯»æ•°æ®ï¼‰

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            required_columns: éœ€è¦æ£€æŸ¥çš„åˆ—ååˆ—è¡¨
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸:
            {
                'exists': bool,  # æ˜¯å¦å…¨éƒ¨å­˜åœ¨
                'missing': list,  # ç¼ºå¤±çš„åˆ—
                'available': list,  # å¯ç”¨çš„åˆ—
                'path': Path  # æ–‡ä»¶è·¯å¾„
            }
        """
        # å¤„ç† returns å’Œ styles çš„è·¨å¸‚åœºå¤ç”¨
        if data_type in ["returns", "styles"]:
            all_path = (
                self.CACHE_DIR
                / f"all_{self.start_date_compact}_{self.end_date_compact}__{data_type}.parquet"
            )
            path = all_path if all_path.exists() else self.get_parquet_path(data_type)
        else:
            path = self.get_parquet_path(data_type)

        if not path.exists():
            return {
                "exists": False,
                "missing": required_columns,
                "available": [],
                "path": path,
            }

        # åªè¯» schemaï¼ˆå…ƒæ•°æ®ï¼‰ï¼Œä¸è¯»æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        try:
            import pyarrow.parquet as pq

            schema = pq.read_schema(path)
            existing_cols = set(schema.names)
        except Exception:
            # å¦‚æœ pyarrow ä¸å¯ç”¨ï¼Œå›é€€åˆ° pandas
            df = pd.read_parquet(path)
            existing_cols = set(df.columns)

        missing_cols = set(required_columns) - existing_cols

        result = {
            "exists": len(missing_cols) == 0,
            "missing": sorted(missing_cols),
            "available": sorted(existing_cols),
            "path": path,
        }

        if verbose and missing_cols:
            print(
                f"âš ï¸  éƒ¨åˆ†å› å­ç¼ºå¤±:\n"
                f"   ç¼ºå¤±: {sorted(missing_cols)}\n"
                f"   å¯ç”¨: {sorted(existing_cols)}"
            )

        return result

    def list_columns(self, data_type: str) -> list[str]:
        """
        åˆ—å‡º Parquet æ–‡ä»¶ä¸­çš„æ‰€æœ‰åˆ—

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†

        Returns:
            åˆ—ååˆ—è¡¨
        """
        path = self.get_parquet_path(data_type)

        if not path.exists():
            return []

        df = pd.read_parquet(path)
        return df.columns.tolist()

    def file_exists(self, data_type: str) -> bool:
        """
        æ£€æŸ¥ Parquet æ–‡ä»¶æ˜¯å¦å­˜åœ¨

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†

        Returns:
            æ˜¯å¦å­˜åœ¨
        """
        return self.get_parquet_path(data_type).exists()

    def get_file_info(self, data_type: str) -> dict:
        """
        è·å–æ–‡ä»¶ä¿¡æ¯

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†

        Returns:
            ä¿¡æ¯å­—å…¸
        """
        path = self.get_parquet_path(data_type)

        if not path.exists():
            return {"path": path, "exists": False}

        df = pd.read_parquet(path)
        stat = path.stat()

        return {
            "path": path,
            "exists": True,
            "size_mb": round(stat.st_size / 1024 / 1024, 2),
            "columns": df.columns.tolist(),
            "shape": df.shape,
            "modified_time": pd.Timestamp(stat.st_mtime, unit="s").isoformat(),
        }

    def delete_file(self, data_type: str) -> bool:
        """
        åˆ é™¤ Parquet æ–‡ä»¶

        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†

        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        path = self.get_parquet_path(data_type)

        if path.exists():
            path.unlink()
            return True
        return False

    def clean_all(self) -> int:
        """
        æ¸…ç†å½“å‰ market+æ—¥æœŸç»„åˆçš„æ‰€æœ‰ cache æ–‡ä»¶

        Returns:
            åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        pattern = f"{self.market}_{self.start_date_compact}_{self.end_date_compact}__*.parquet"
        files = list(self.CACHE_DIR.glob(pattern))

        for f in files:
            f.unlink()

        return len(files)

    def list_cache_files(self) -> list[Path]:
        """
        åˆ—å‡º cache ç›®å½•ä¸­æ‰€æœ‰ Parquet æ–‡ä»¶

        Returns:
            æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        return list(self.CACHE_DIR.glob("*.parquet"))
